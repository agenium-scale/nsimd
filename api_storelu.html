<!--

Copyright (c) 2021 Agenium Scale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

-->

<!-- This file has been auto-generated -->

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NSIMD documentation</title>
    <style type="text/css">
      body {
        /*margin:40px auto;*/
        margin:10px auto;
        /*max-width:650px;*/
        max-width:800px;
        /*line-height:1.6;*/
        line-height:1.4;
        /*font-size:18px;*/
        color:#444;
        padding: 0 10px;
      }
      h1,h2,h3 {
        line-height: 1.2;
      }
      table {
        border-collapse: collapse;
        border: 0px solid gray;
        width: 100%;
      }
      th, td {
        border: 2px solid gray;
        padding: 0px 1em 0px 1em;
      }
    </style>
    <!-- https://www.mathjax.org/#gettingstarted -->
    <script src="assets/polyfill.min.js"></script>
    <script id="MathJax-script" async src="assets/tex-svg.js"></script>
    <!-- Highlight.js -->
    <link rel="stylesheet" href= "assets/highlight.js.default.min.css">
    <script src="assets/highlight.min.js"></script>
    <script src="assets/cpp.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
<body>

<div style="text-align: center; margin-bottom: 1em;">
  <img src="img/logo.svg">
  <hr>
</div>
<div style="text-align: center; margin-bottom: 1em;">
  <b>NSIMD documentation</b>
</div>
<div style="text-align: center; margin-bottom: 1em;">
  <a href="index.html">Index</a> |
  <a href="tutorial.html">Tutorial</a> |
  <a href="faq.html">FAQ</a> |
  <a href="contribute.html">Contribute</a> |
  <a href="overview.html">API overview</a> |
  <a href="api.html">API reference</a> |
  <a href="wrapped_intrinsics.html">Wrapped intrinsics</a> |
  <a href="modules.html">Modules</a>
  <hr>
</div>

<h1>Store vector of logicals</h1>
<h2>Description</h2>
<p>Store SIMD vector of booleans into unaligned memory. True is stored as 1 and False as 0.</p>
<h2>C base API (generic)</h2>
<pre class="c"><code>#define vstorelu(a0, a1, type)
#define vstorelu_e(a0, a1, type, simd_ext)</code></pre>
<h2>C advanced API (generic, requires C11)</h2>
<pre class="c"><code>#define nsimd_storelu(a0, a1)</code></pre>
<h2>C++ base API (generic)</h2>
<pre class="c++"><code>template &lt;NSIMD_CONCEPT_VALUE_TYPE T&gt; void NSIMD_VECTORCALL storelu(T* a0, typename simd_traits&lt;T, NSIMD_SIMD&gt;::simd_vectorl a1, T);</code></pre>
<h2>C++ advanced API</h2>
<pre class="c++"><code>template &lt;NSIMD_CONCEPT_VALUE_TYPE T, NSIMD_CONCEPT_SIMD_EXT SimdExt&gt; void storelu(T* a0, packl&lt;T, 1, SimdExt&gt; const&amp; a1);
template &lt;NSIMD_CONCEPT_VALUE_TYPE T, int N, NSIMD_CONCEPT_SIMD_EXT SimdExt&gt; void storelu(T* a0, packl&lt;T, N, SimdExt&gt; const&amp; a1);</code></pre>
<h2>C base API (architecture specifics)</h2>
<h3>SSE2</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sse2_f64(f64* a0, nsimd_sse2_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_f32(f32* a0, nsimd_sse2_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_f16(f16* a0, nsimd_sse2_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_i64(i64* a0, nsimd_sse2_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_i32(i32* a0, nsimd_sse2_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_i16(i16* a0, nsimd_sse2_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_i8(i8* a0, nsimd_sse2_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_u64(u64* a0, nsimd_sse2_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_u32(u32* a0, nsimd_sse2_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_u16(u16* a0, nsimd_sse2_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse2_u8(u8* a0, nsimd_sse2_vlu8 a1);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_f64(f64* a0, nsimd_avx512_skylake_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_f32(f32* a0, nsimd_avx512_skylake_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_f16(f16* a0, nsimd_avx512_skylake_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_i64(i64* a0, nsimd_avx512_skylake_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_i32(i32* a0, nsimd_avx512_skylake_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_i16(i16* a0, nsimd_avx512_skylake_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_i8(i8* a0, nsimd_avx512_skylake_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_u64(u64* a0, nsimd_avx512_skylake_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_u32(u32* a0, nsimd_avx512_skylake_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_u16(u16* a0, nsimd_avx512_skylake_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_skylake_u8(u8* a0, nsimd_avx512_skylake_vlu8 a1);</code></pre>
<h3>VSX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_vsx_f64(f64* a0, nsimd_vsx_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_f32(f32* a0, nsimd_vsx_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_f16(f16* a0, nsimd_vsx_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_i64(i64* a0, nsimd_vsx_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_i32(i32* a0, nsimd_vsx_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_i16(i16* a0, nsimd_vsx_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_i8(i8* a0, nsimd_vsx_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_u64(u64* a0, nsimd_vsx_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_u32(u32* a0, nsimd_vsx_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_u16(u16* a0, nsimd_vsx_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vsx_u8(u8* a0, nsimd_vsx_vlu8 a1);</code></pre>
<h3>SVE512</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve512_f64(f64* a0, nsimd_sve512_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_f32(f32* a0, nsimd_sve512_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_f16(f16* a0, nsimd_sve512_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_i64(i64* a0, nsimd_sve512_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_i32(i32* a0, nsimd_sve512_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_i16(i16* a0, nsimd_sve512_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_i8(i8* a0, nsimd_sve512_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_u64(u64* a0, nsimd_sve512_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_u32(u32* a0, nsimd_sve512_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_u16(u16* a0, nsimd_sve512_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve512_u8(u8* a0, nsimd_sve512_vlu8 a1);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_f64(f64* a0, nsimd_avx512_knl_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_f32(f32* a0, nsimd_avx512_knl_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_f16(f16* a0, nsimd_avx512_knl_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_i64(i64* a0, nsimd_avx512_knl_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_i32(i32* a0, nsimd_avx512_knl_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_i16(i16* a0, nsimd_avx512_knl_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_i8(i8* a0, nsimd_avx512_knl_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_u64(u64* a0, nsimd_avx512_knl_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_u32(u32* a0, nsimd_avx512_knl_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_u16(u16* a0, nsimd_avx512_knl_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx512_knl_u8(u8* a0, nsimd_avx512_knl_vlu8 a1);</code></pre>
<h3>VMX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_vmx_f64(f64* a0, nsimd_vmx_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_f32(f32* a0, nsimd_vmx_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_f16(f16* a0, nsimd_vmx_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_i64(i64* a0, nsimd_vmx_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_i32(i32* a0, nsimd_vmx_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_i16(i16* a0, nsimd_vmx_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_i8(i8* a0, nsimd_vmx_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_u64(u64* a0, nsimd_vmx_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_u32(u32* a0, nsimd_vmx_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_u16(u16* a0, nsimd_vmx_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_vmx_u8(u8* a0, nsimd_vmx_vlu8 a1);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_neon128_f64(f64* a0, nsimd_neon128_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_f32(f32* a0, nsimd_neon128_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_f16(f16* a0, nsimd_neon128_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_i64(i64* a0, nsimd_neon128_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_i32(i32* a0, nsimd_neon128_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_i16(i16* a0, nsimd_neon128_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_i8(i8* a0, nsimd_neon128_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_u64(u64* a0, nsimd_neon128_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_u32(u32* a0, nsimd_neon128_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_u16(u16* a0, nsimd_neon128_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_neon128_u8(u8* a0, nsimd_neon128_vlu8 a1);</code></pre>
<h3>SVE256</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve256_f64(f64* a0, nsimd_sve256_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_f32(f32* a0, nsimd_sve256_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_f16(f16* a0, nsimd_sve256_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_i64(i64* a0, nsimd_sve256_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_i32(i32* a0, nsimd_sve256_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_i16(i16* a0, nsimd_sve256_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_i8(i8* a0, nsimd_sve256_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_u64(u64* a0, nsimd_sve256_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_u32(u32* a0, nsimd_sve256_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_u16(u16* a0, nsimd_sve256_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve256_u8(u8* a0, nsimd_sve256_vlu8 a1);</code></pre>
<h3>SVE2048</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve2048_f64(f64* a0, nsimd_sve2048_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_f32(f32* a0, nsimd_sve2048_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_f16(f16* a0, nsimd_sve2048_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_i64(i64* a0, nsimd_sve2048_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_i32(i32* a0, nsimd_sve2048_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_i16(i16* a0, nsimd_sve2048_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_i8(i8* a0, nsimd_sve2048_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_u64(u64* a0, nsimd_sve2048_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_u32(u32* a0, nsimd_sve2048_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_u16(u16* a0, nsimd_sve2048_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve2048_u8(u8* a0, nsimd_sve2048_vlu8 a1);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_avx2_f64(f64* a0, nsimd_avx2_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_f32(f32* a0, nsimd_avx2_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_f16(f16* a0, nsimd_avx2_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_i64(i64* a0, nsimd_avx2_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_i32(i32* a0, nsimd_avx2_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_i16(i16* a0, nsimd_avx2_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_i8(i8* a0, nsimd_avx2_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_u64(u64* a0, nsimd_avx2_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_u32(u32* a0, nsimd_avx2_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_u16(u16* a0, nsimd_avx2_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx2_u8(u8* a0, nsimd_avx2_vlu8 a1);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_avx_f64(f64* a0, nsimd_avx_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_f32(f32* a0, nsimd_avx_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_f16(f16* a0, nsimd_avx_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_i64(i64* a0, nsimd_avx_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_i32(i32* a0, nsimd_avx_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_i16(i16* a0, nsimd_avx_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_i8(i8* a0, nsimd_avx_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_u64(u64* a0, nsimd_avx_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_u32(u32* a0, nsimd_avx_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_u16(u16* a0, nsimd_avx_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_avx_u8(u8* a0, nsimd_avx_vlu8 a1);</code></pre>
<h3>SVE1024</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve1024_f64(f64* a0, nsimd_sve1024_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_f32(f32* a0, nsimd_sve1024_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_f16(f16* a0, nsimd_sve1024_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_i64(i64* a0, nsimd_sve1024_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_i32(i32* a0, nsimd_sve1024_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_i16(i16* a0, nsimd_sve1024_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_i8(i8* a0, nsimd_sve1024_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_u64(u64* a0, nsimd_sve1024_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_u32(u32* a0, nsimd_sve1024_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_u16(u16* a0, nsimd_sve1024_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve1024_u8(u8* a0, nsimd_sve1024_vlu8 a1);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve_f64(f64* a0, nsimd_sve_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_f32(f32* a0, nsimd_sve_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_f16(f16* a0, nsimd_sve_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_i64(i64* a0, nsimd_sve_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_i32(i32* a0, nsimd_sve_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_i16(i16* a0, nsimd_sve_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_i8(i8* a0, nsimd_sve_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_u64(u64* a0, nsimd_sve_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_u32(u32* a0, nsimd_sve_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_u16(u16* a0, nsimd_sve_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve_u8(u8* a0, nsimd_sve_vlu8 a1);</code></pre>
<h3>AARCH64</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_aarch64_f64(f64* a0, nsimd_aarch64_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_f32(f32* a0, nsimd_aarch64_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_f16(f16* a0, nsimd_aarch64_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_i64(i64* a0, nsimd_aarch64_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_i32(i32* a0, nsimd_aarch64_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_i16(i16* a0, nsimd_aarch64_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_i8(i8* a0, nsimd_aarch64_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_u64(u64* a0, nsimd_aarch64_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_u32(u32* a0, nsimd_aarch64_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_u16(u16* a0, nsimd_aarch64_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_aarch64_u8(u8* a0, nsimd_aarch64_vlu8 a1);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_cpu_f64(f64* a0, nsimd_cpu_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_f32(f32* a0, nsimd_cpu_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_f16(f16* a0, nsimd_cpu_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_i64(i64* a0, nsimd_cpu_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_i32(i32* a0, nsimd_cpu_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_i16(i16* a0, nsimd_cpu_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_i8(i8* a0, nsimd_cpu_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_u64(u64* a0, nsimd_cpu_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_u32(u32* a0, nsimd_cpu_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_u16(u16* a0, nsimd_cpu_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_cpu_u8(u8* a0, nsimd_cpu_vlu8 a1);</code></pre>
<h3>SVE128</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sve128_f64(f64* a0, nsimd_sve128_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_f32(f32* a0, nsimd_sve128_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_f16(f16* a0, nsimd_sve128_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_i64(i64* a0, nsimd_sve128_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_i32(i32* a0, nsimd_sve128_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_i16(i16* a0, nsimd_sve128_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_i8(i8* a0, nsimd_sve128_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_u64(u64* a0, nsimd_sve128_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_u32(u32* a0, nsimd_sve128_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_u16(u16* a0, nsimd_sve128_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sve128_u8(u8* a0, nsimd_sve128_vlu8 a1);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>void NSIMD_VECTORCALL nsimd_storelu_sse42_f64(f64* a0, nsimd_sse42_vlf64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_f32(f32* a0, nsimd_sse42_vlf32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_f16(f16* a0, nsimd_sse42_vlf16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_i64(i64* a0, nsimd_sse42_vli64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_i32(i32* a0, nsimd_sse42_vli32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_i16(i16* a0, nsimd_sse42_vli16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_i8(i8* a0, nsimd_sse42_vli8 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_u64(u64* a0, nsimd_sse42_vlu64 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_u32(u32* a0, nsimd_sse42_vlu32 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_u16(u16* a0, nsimd_sse42_vlu16 a1);
void NSIMD_VECTORCALL nsimd_storelu_sse42_u8(u8* a0, nsimd_sse42_vlu8 a1);</code></pre>
<h2>C++ base API (architecture specifics)</h2>
<h3>SSE2</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sse2_vlf64 a1, f64, sse2);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sse2_vlf32 a1, f32, sse2);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sse2_vlf16 a1, f16, sse2);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sse2_vli64 a1, i64, sse2);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sse2_vli32 a1, i32, sse2);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sse2_vli16 a1, i16, sse2);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sse2_vli8 a1, i8, sse2);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sse2_vlu64 a1, u64, sse2);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sse2_vlu32 a1, u32, sse2);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sse2_vlu16 a1, u16, sse2);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sse2_vlu8 a1, u8, sse2);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_avx512_skylake_vlf64 a1, f64, avx512_skylake);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_avx512_skylake_vlf32 a1, f32, avx512_skylake);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_avx512_skylake_vlf16 a1, f16, avx512_skylake);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_avx512_skylake_vli64 a1, i64, avx512_skylake);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_avx512_skylake_vli32 a1, i32, avx512_skylake);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_avx512_skylake_vli16 a1, i16, avx512_skylake);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_avx512_skylake_vli8 a1, i8, avx512_skylake);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_avx512_skylake_vlu64 a1, u64, avx512_skylake);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_avx512_skylake_vlu32 a1, u32, avx512_skylake);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_avx512_skylake_vlu16 a1, u16, avx512_skylake);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_avx512_skylake_vlu8 a1, u8, avx512_skylake);</code></pre>
<h3>VSX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_vsx_vlf64 a1, f64, vsx);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_vsx_vlf32 a1, f32, vsx);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_vsx_vlf16 a1, f16, vsx);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_vsx_vli64 a1, i64, vsx);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_vsx_vli32 a1, i32, vsx);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_vsx_vli16 a1, i16, vsx);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_vsx_vli8 a1, i8, vsx);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_vsx_vlu64 a1, u64, vsx);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_vsx_vlu32 a1, u32, vsx);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_vsx_vlu16 a1, u16, vsx);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_vsx_vlu8 a1, u8, vsx);</code></pre>
<h3>SVE512</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve512_vlf64 a1, f64, sve512);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve512_vlf32 a1, f32, sve512);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve512_vlf16 a1, f16, sve512);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve512_vli64 a1, i64, sve512);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve512_vli32 a1, i32, sve512);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve512_vli16 a1, i16, sve512);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve512_vli8 a1, i8, sve512);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve512_vlu64 a1, u64, sve512);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve512_vlu32 a1, u32, sve512);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve512_vlu16 a1, u16, sve512);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve512_vlu8 a1, u8, sve512);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_avx512_knl_vlf64 a1, f64, avx512_knl);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_avx512_knl_vlf32 a1, f32, avx512_knl);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_avx512_knl_vlf16 a1, f16, avx512_knl);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_avx512_knl_vli64 a1, i64, avx512_knl);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_avx512_knl_vli32 a1, i32, avx512_knl);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_avx512_knl_vli16 a1, i16, avx512_knl);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_avx512_knl_vli8 a1, i8, avx512_knl);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_avx512_knl_vlu64 a1, u64, avx512_knl);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_avx512_knl_vlu32 a1, u32, avx512_knl);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_avx512_knl_vlu16 a1, u16, avx512_knl);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_avx512_knl_vlu8 a1, u8, avx512_knl);</code></pre>
<h3>VMX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_vmx_vlf64 a1, f64, vmx);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_vmx_vlf32 a1, f32, vmx);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_vmx_vlf16 a1, f16, vmx);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_vmx_vli64 a1, i64, vmx);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_vmx_vli32 a1, i32, vmx);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_vmx_vli16 a1, i16, vmx);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_vmx_vli8 a1, i8, vmx);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_vmx_vlu64 a1, u64, vmx);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_vmx_vlu32 a1, u32, vmx);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_vmx_vlu16 a1, u16, vmx);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_vmx_vlu8 a1, u8, vmx);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_neon128_vlf64 a1, f64, neon128);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_neon128_vlf32 a1, f32, neon128);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_neon128_vlf16 a1, f16, neon128);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_neon128_vli64 a1, i64, neon128);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_neon128_vli32 a1, i32, neon128);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_neon128_vli16 a1, i16, neon128);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_neon128_vli8 a1, i8, neon128);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_neon128_vlu64 a1, u64, neon128);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_neon128_vlu32 a1, u32, neon128);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_neon128_vlu16 a1, u16, neon128);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_neon128_vlu8 a1, u8, neon128);</code></pre>
<h3>SVE256</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve256_vlf64 a1, f64, sve256);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve256_vlf32 a1, f32, sve256);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve256_vlf16 a1, f16, sve256);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve256_vli64 a1, i64, sve256);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve256_vli32 a1, i32, sve256);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve256_vli16 a1, i16, sve256);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve256_vli8 a1, i8, sve256);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve256_vlu64 a1, u64, sve256);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve256_vlu32 a1, u32, sve256);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve256_vlu16 a1, u16, sve256);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve256_vlu8 a1, u8, sve256);</code></pre>
<h3>SVE2048</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve2048_vlf64 a1, f64, sve2048);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve2048_vlf32 a1, f32, sve2048);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve2048_vlf16 a1, f16, sve2048);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve2048_vli64 a1, i64, sve2048);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve2048_vli32 a1, i32, sve2048);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve2048_vli16 a1, i16, sve2048);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve2048_vli8 a1, i8, sve2048);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve2048_vlu64 a1, u64, sve2048);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve2048_vlu32 a1, u32, sve2048);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve2048_vlu16 a1, u16, sve2048);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve2048_vlu8 a1, u8, sve2048);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_avx2_vlf64 a1, f64, avx2);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_avx2_vlf32 a1, f32, avx2);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_avx2_vlf16 a1, f16, avx2);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_avx2_vli64 a1, i64, avx2);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_avx2_vli32 a1, i32, avx2);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_avx2_vli16 a1, i16, avx2);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_avx2_vli8 a1, i8, avx2);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_avx2_vlu64 a1, u64, avx2);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_avx2_vlu32 a1, u32, avx2);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_avx2_vlu16 a1, u16, avx2);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_avx2_vlu8 a1, u8, avx2);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_avx_vlf64 a1, f64, avx);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_avx_vlf32 a1, f32, avx);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_avx_vlf16 a1, f16, avx);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_avx_vli64 a1, i64, avx);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_avx_vli32 a1, i32, avx);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_avx_vli16 a1, i16, avx);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_avx_vli8 a1, i8, avx);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_avx_vlu64 a1, u64, avx);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_avx_vlu32 a1, u32, avx);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_avx_vlu16 a1, u16, avx);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_avx_vlu8 a1, u8, avx);</code></pre>
<h3>SVE1024</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve1024_vlf64 a1, f64, sve1024);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve1024_vlf32 a1, f32, sve1024);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve1024_vlf16 a1, f16, sve1024);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve1024_vli64 a1, i64, sve1024);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve1024_vli32 a1, i32, sve1024);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve1024_vli16 a1, i16, sve1024);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve1024_vli8 a1, i8, sve1024);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve1024_vlu64 a1, u64, sve1024);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve1024_vlu32 a1, u32, sve1024);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve1024_vlu16 a1, u16, sve1024);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve1024_vlu8 a1, u8, sve1024);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve_vlf64 a1, f64, sve);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve_vlf32 a1, f32, sve);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve_vlf16 a1, f16, sve);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve_vli64 a1, i64, sve);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve_vli32 a1, i32, sve);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve_vli16 a1, i16, sve);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve_vli8 a1, i8, sve);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve_vlu64 a1, u64, sve);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve_vlu32 a1, u32, sve);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve_vlu16 a1, u16, sve);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve_vlu8 a1, u8, sve);</code></pre>
<h3>AARCH64</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_aarch64_vlf64 a1, f64, aarch64);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_aarch64_vlf32 a1, f32, aarch64);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_aarch64_vlf16 a1, f16, aarch64);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_aarch64_vli64 a1, i64, aarch64);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_aarch64_vli32 a1, i32, aarch64);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_aarch64_vli16 a1, i16, aarch64);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_aarch64_vli8 a1, i8, aarch64);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_aarch64_vlu64 a1, u64, aarch64);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_aarch64_vlu32 a1, u32, aarch64);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_aarch64_vlu16 a1, u16, aarch64);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_aarch64_vlu8 a1, u8, aarch64);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_cpu_vlf64 a1, f64, cpu);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_cpu_vlf32 a1, f32, cpu);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_cpu_vlf16 a1, f16, cpu);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_cpu_vli64 a1, i64, cpu);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_cpu_vli32 a1, i32, cpu);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_cpu_vli16 a1, i16, cpu);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_cpu_vli8 a1, i8, cpu);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_cpu_vlu64 a1, u64, cpu);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_cpu_vlu32 a1, u32, cpu);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_cpu_vlu16 a1, u16, cpu);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_cpu_vlu8 a1, u8, cpu);</code></pre>
<h3>SVE128</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sve128_vlf64 a1, f64, sve128);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sve128_vlf32 a1, f32, sve128);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sve128_vlf16 a1, f16, sve128);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sve128_vli64 a1, i64, sve128);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sve128_vli32 a1, i32, sve128);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sve128_vli16 a1, i16, sve128);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sve128_vli8 a1, i8, sve128);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sve128_vlu64 a1, u64, sve128);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sve128_vlu32 a1, u32, sve128);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sve128_vlu16 a1, u16, sve128);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sve128_vlu8 a1, u8, sve128);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>void NSIMD_VECTORCALL storelu(f64* a0, nsimd_sse42_vlf64 a1, f64, sse42);
void NSIMD_VECTORCALL storelu(f32* a0, nsimd_sse42_vlf32 a1, f32, sse42);
void NSIMD_VECTORCALL storelu(f16* a0, nsimd_sse42_vlf16 a1, f16, sse42);
void NSIMD_VECTORCALL storelu(i64* a0, nsimd_sse42_vli64 a1, i64, sse42);
void NSIMD_VECTORCALL storelu(i32* a0, nsimd_sse42_vli32 a1, i32, sse42);
void NSIMD_VECTORCALL storelu(i16* a0, nsimd_sse42_vli16 a1, i16, sse42);
void NSIMD_VECTORCALL storelu(i8* a0, nsimd_sse42_vli8 a1, i8, sse42);
void NSIMD_VECTORCALL storelu(u64* a0, nsimd_sse42_vlu64 a1, u64, sse42);
void NSIMD_VECTORCALL storelu(u32* a0, nsimd_sse42_vlu32 a1, u32, sse42);
void NSIMD_VECTORCALL storelu(u16* a0, nsimd_sse42_vlu16 a1, u16, sse42);
void NSIMD_VECTORCALL storelu(u8* a0, nsimd_sse42_vlu8 a1, u8, sse42);</code></pre>
  </body>
</html>
