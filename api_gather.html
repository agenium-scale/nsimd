<!--

Copyright (c) 2020 Agenium Scale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

-->

<!-- This file has been auto-generated -->

<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>NSIMD documentation</title>
    <style type="text/css">
      body {
        /*margin:40px auto;*/
        margin:10px auto;
        /*max-width:650px;*/
        max-width:800px;
        /*line-height:1.6;*/
        line-height:1.4;
        /*font-size:18px;*/
        color:#444;
        padding: 0 10px;
      }
      h1,h2,h3 {
        line-height: 1.2;
      }
      table {
        border-collapse: collapse;
        border: 0px solid gray;
        width: 100%;
      }
      th, td {
        border: 2px solid gray;
        padding: 0px 1em 0px 1em;
      }
    </style>
    <!-- https://www.mathjax.org/#gettingstarted -->
    <script src="assets/polyfill.min.js"></script>
    <script id="MathJax-script" async src="assets/tex-mml-chtml.js">
    </script>
    <!-- Highlight.js -->
    <link rel="stylesheet" href= "assets/highlight.js.default.min.css">
    <script src="assets/highlight.min.js"></script>
    <script src="assets/cpp.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </head>
<body>

<div style="text-align: center; margin-bottom: 1em;">
  <img src="img/logo.svg">
  <hr>
</div>
<div style="text-align: center; margin-bottom: 1em;">
  <b>NSIMD documentation</b>
</div>
<div style="text-align: center; margin-bottom: 1em;">
  <a href="index.html">Index</a> |
  <a href="tutorial.html">Tutorial</a> |
  <a href="faq.html">FAQ</a> |
  <a href="contribute.html">Contribute</a> |
  <a href="overview.html">API overview</a> |
  <a href="api.html">API reference</a> |
  <a href="wrapped_intrinsics.html">Wrapped intrinsics</a> |
  <a href="modules.html">Modules</a>
  <hr>
</div>

<h1>Gather elements from memory into a SIMD vector</h1>
<h2>Description</h2>
<p>Gather elements from memory with base address given as first argument and offsets given as second argument.</p>
<h2>C base API (generic)</h2>
<pre class="c"><code>#define vgather(a0, a1, type)
#define vgather_e(a0, a1, type, simd_ext)</code></pre>
<h2>C++ base API (generic)</h2>
<pre class="c++"><code>template &lt;NSIMD_CONCEPT_VALUE_TYPE T&gt; typename simd_traits&lt;T, NSIMD_SIMD&gt;::simd_vector NSIMD_VECTORCALL gather(T const* a0, typename simd_traits&lt;typename traits&lt;T&gt;::itype, NSIMD_SIMD&gt;::simd_vector a1, T);</code></pre>
<h2>C++ advanced API</h2>
<pre class="c++"><code>template &lt;NSIMD_CONCEPT_VALUE_TYPE T, NSIMD_CONCEPT_SIMD_EXT SimdExt&gt; pack&lt;T, 1, SimdExt&gt; gather(T const* a0, pack&lt;typename traits&lt;T&gt;::itype, 1, SimdExt&gt; const&amp; a1);
template &lt;NSIMD_CONCEPT_VALUE_TYPE T, int N, NSIMD_CONCEPT_SIMD_EXT SimdExt&gt; pack&lt;T, N, SimdExt&gt; gather(T const* a0, pack&lt;typename traits&lt;T&gt;::itype, N, SimdExt&gt; const&amp; a1);</code></pre>
<h2>C base API (architecture specifics)</h2>
<h3>SVE128</h3>
<pre class="c"><code>nsimd_sve128_vf64 NSIMD_VECTORCALL nsimd_gather_sve128_f64(f64 const* a0, nsimd_sve128_vi64 a1);
nsimd_sve128_vf32 NSIMD_VECTORCALL nsimd_gather_sve128_f32(f32 const* a0, nsimd_sve128_vi32 a1);
nsimd_sve128_vf16 NSIMD_VECTORCALL nsimd_gather_sve128_f16(f16 const* a0, nsimd_sve128_vi16 a1);
nsimd_sve128_vi16 NSIMD_VECTORCALL nsimd_gather_sve128_i16(i16 const* a0, nsimd_sve128_vi16 a1);
nsimd_sve128_vu16 NSIMD_VECTORCALL nsimd_gather_sve128_u16(u16 const* a0, nsimd_sve128_vi16 a1);
nsimd_sve128_vu32 NSIMD_VECTORCALL nsimd_gather_sve128_u32(u32 const* a0, nsimd_sve128_vi32 a1);
nsimd_sve128_vi32 NSIMD_VECTORCALL nsimd_gather_sve128_i32(i32 const* a0, nsimd_sve128_vi32 a1);
nsimd_sve128_vi64 NSIMD_VECTORCALL nsimd_gather_sve128_i64(i64 const* a0, nsimd_sve128_vi64 a1);
nsimd_sve128_vu64 NSIMD_VECTORCALL nsimd_gather_sve128_u64(u64 const* a0, nsimd_sve128_vi64 a1);</code></pre>
<h3>POWER7</h3>
<pre class="c"><code>nsimd_power7_vf64 NSIMD_VECTORCALL nsimd_gather_power7_f64(f64 const* a0, nsimd_power7_vi64 a1);
nsimd_power7_vf32 NSIMD_VECTORCALL nsimd_gather_power7_f32(f32 const* a0, nsimd_power7_vi32 a1);
nsimd_power7_vf16 NSIMD_VECTORCALL nsimd_gather_power7_f16(f16 const* a0, nsimd_power7_vi16 a1);
nsimd_power7_vi16 NSIMD_VECTORCALL nsimd_gather_power7_i16(i16 const* a0, nsimd_power7_vi16 a1);
nsimd_power7_vu16 NSIMD_VECTORCALL nsimd_gather_power7_u16(u16 const* a0, nsimd_power7_vi16 a1);
nsimd_power7_vu32 NSIMD_VECTORCALL nsimd_gather_power7_u32(u32 const* a0, nsimd_power7_vi32 a1);
nsimd_power7_vi32 NSIMD_VECTORCALL nsimd_gather_power7_i32(i32 const* a0, nsimd_power7_vi32 a1);
nsimd_power7_vi64 NSIMD_VECTORCALL nsimd_gather_power7_i64(i64 const* a0, nsimd_power7_vi64 a1);
nsimd_power7_vu64 NSIMD_VECTORCALL nsimd_gather_power7_u64(u64 const* a0, nsimd_power7_vi64 a1);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>nsimd_avx_vf64 NSIMD_VECTORCALL nsimd_gather_avx_f64(f64 const* a0, nsimd_avx_vi64 a1);
nsimd_avx_vf32 NSIMD_VECTORCALL nsimd_gather_avx_f32(f32 const* a0, nsimd_avx_vi32 a1);
nsimd_avx_vf16 NSIMD_VECTORCALL nsimd_gather_avx_f16(f16 const* a0, nsimd_avx_vi16 a1);
nsimd_avx_vi16 NSIMD_VECTORCALL nsimd_gather_avx_i16(i16 const* a0, nsimd_avx_vi16 a1);
nsimd_avx_vu16 NSIMD_VECTORCALL nsimd_gather_avx_u16(u16 const* a0, nsimd_avx_vi16 a1);
nsimd_avx_vu32 NSIMD_VECTORCALL nsimd_gather_avx_u32(u32 const* a0, nsimd_avx_vi32 a1);
nsimd_avx_vi32 NSIMD_VECTORCALL nsimd_gather_avx_i32(i32 const* a0, nsimd_avx_vi32 a1);
nsimd_avx_vi64 NSIMD_VECTORCALL nsimd_gather_avx_i64(i64 const* a0, nsimd_avx_vi64 a1);
nsimd_avx_vu64 NSIMD_VECTORCALL nsimd_gather_avx_u64(u64 const* a0, nsimd_avx_vi64 a1);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>nsimd_sse42_vf64 NSIMD_VECTORCALL nsimd_gather_sse42_f64(f64 const* a0, nsimd_sse42_vi64 a1);
nsimd_sse42_vf32 NSIMD_VECTORCALL nsimd_gather_sse42_f32(f32 const* a0, nsimd_sse42_vi32 a1);
nsimd_sse42_vf16 NSIMD_VECTORCALL nsimd_gather_sse42_f16(f16 const* a0, nsimd_sse42_vi16 a1);
nsimd_sse42_vi16 NSIMD_VECTORCALL nsimd_gather_sse42_i16(i16 const* a0, nsimd_sse42_vi16 a1);
nsimd_sse42_vu16 NSIMD_VECTORCALL nsimd_gather_sse42_u16(u16 const* a0, nsimd_sse42_vi16 a1);
nsimd_sse42_vu32 NSIMD_VECTORCALL nsimd_gather_sse42_u32(u32 const* a0, nsimd_sse42_vi32 a1);
nsimd_sse42_vi32 NSIMD_VECTORCALL nsimd_gather_sse42_i32(i32 const* a0, nsimd_sse42_vi32 a1);
nsimd_sse42_vi64 NSIMD_VECTORCALL nsimd_gather_sse42_i64(i64 const* a0, nsimd_sse42_vi64 a1);
nsimd_sse42_vu64 NSIMD_VECTORCALL nsimd_gather_sse42_u64(u64 const* a0, nsimd_sse42_vi64 a1);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>nsimd_neon128_vf64 NSIMD_VECTORCALL nsimd_gather_neon128_f64(f64 const* a0, nsimd_neon128_vi64 a1);
nsimd_neon128_vf32 NSIMD_VECTORCALL nsimd_gather_neon128_f32(f32 const* a0, nsimd_neon128_vi32 a1);
nsimd_neon128_vf16 NSIMD_VECTORCALL nsimd_gather_neon128_f16(f16 const* a0, nsimd_neon128_vi16 a1);
nsimd_neon128_vi16 NSIMD_VECTORCALL nsimd_gather_neon128_i16(i16 const* a0, nsimd_neon128_vi16 a1);
nsimd_neon128_vu16 NSIMD_VECTORCALL nsimd_gather_neon128_u16(u16 const* a0, nsimd_neon128_vi16 a1);
nsimd_neon128_vu32 NSIMD_VECTORCALL nsimd_gather_neon128_u32(u32 const* a0, nsimd_neon128_vi32 a1);
nsimd_neon128_vi32 NSIMD_VECTORCALL nsimd_gather_neon128_i32(i32 const* a0, nsimd_neon128_vi32 a1);
nsimd_neon128_vi64 NSIMD_VECTORCALL nsimd_gather_neon128_i64(i64 const* a0, nsimd_neon128_vi64 a1);
nsimd_neon128_vu64 NSIMD_VECTORCALL nsimd_gather_neon128_u64(u64 const* a0, nsimd_neon128_vi64 a1);</code></pre>
<h3>POWER8</h3>
<pre class="c"><code>nsimd_power8_vf64 NSIMD_VECTORCALL nsimd_gather_power8_f64(f64 const* a0, nsimd_power8_vi64 a1);
nsimd_power8_vf32 NSIMD_VECTORCALL nsimd_gather_power8_f32(f32 const* a0, nsimd_power8_vi32 a1);
nsimd_power8_vf16 NSIMD_VECTORCALL nsimd_gather_power8_f16(f16 const* a0, nsimd_power8_vi16 a1);
nsimd_power8_vi16 NSIMD_VECTORCALL nsimd_gather_power8_i16(i16 const* a0, nsimd_power8_vi16 a1);
nsimd_power8_vu16 NSIMD_VECTORCALL nsimd_gather_power8_u16(u16 const* a0, nsimd_power8_vi16 a1);
nsimd_power8_vu32 NSIMD_VECTORCALL nsimd_gather_power8_u32(u32 const* a0, nsimd_power8_vi32 a1);
nsimd_power8_vi32 NSIMD_VECTORCALL nsimd_gather_power8_i32(i32 const* a0, nsimd_power8_vi32 a1);
nsimd_power8_vi64 NSIMD_VECTORCALL nsimd_gather_power8_i64(i64 const* a0, nsimd_power8_vi64 a1);
nsimd_power8_vu64 NSIMD_VECTORCALL nsimd_gather_power8_u64(u64 const* a0, nsimd_power8_vi64 a1);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>nsimd_avx512_knl_vf64 NSIMD_VECTORCALL nsimd_gather_avx512_knl_f64(f64 const* a0, nsimd_avx512_knl_vi64 a1);
nsimd_avx512_knl_vf32 NSIMD_VECTORCALL nsimd_gather_avx512_knl_f32(f32 const* a0, nsimd_avx512_knl_vi32 a1);
nsimd_avx512_knl_vf16 NSIMD_VECTORCALL nsimd_gather_avx512_knl_f16(f16 const* a0, nsimd_avx512_knl_vi16 a1);
nsimd_avx512_knl_vi16 NSIMD_VECTORCALL nsimd_gather_avx512_knl_i16(i16 const* a0, nsimd_avx512_knl_vi16 a1);
nsimd_avx512_knl_vu16 NSIMD_VECTORCALL nsimd_gather_avx512_knl_u16(u16 const* a0, nsimd_avx512_knl_vi16 a1);
nsimd_avx512_knl_vu32 NSIMD_VECTORCALL nsimd_gather_avx512_knl_u32(u32 const* a0, nsimd_avx512_knl_vi32 a1);
nsimd_avx512_knl_vi32 NSIMD_VECTORCALL nsimd_gather_avx512_knl_i32(i32 const* a0, nsimd_avx512_knl_vi32 a1);
nsimd_avx512_knl_vi64 NSIMD_VECTORCALL nsimd_gather_avx512_knl_i64(i64 const* a0, nsimd_avx512_knl_vi64 a1);
nsimd_avx512_knl_vu64 NSIMD_VECTORCALL nsimd_gather_avx512_knl_u64(u64 const* a0, nsimd_avx512_knl_vi64 a1);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>nsimd_cpu_vf64 NSIMD_VECTORCALL nsimd_gather_cpu_f64(f64 const* a0, nsimd_cpu_vi64 a1);
nsimd_cpu_vf32 NSIMD_VECTORCALL nsimd_gather_cpu_f32(f32 const* a0, nsimd_cpu_vi32 a1);
nsimd_cpu_vf16 NSIMD_VECTORCALL nsimd_gather_cpu_f16(f16 const* a0, nsimd_cpu_vi16 a1);
nsimd_cpu_vi16 NSIMD_VECTORCALL nsimd_gather_cpu_i16(i16 const* a0, nsimd_cpu_vi16 a1);
nsimd_cpu_vu16 NSIMD_VECTORCALL nsimd_gather_cpu_u16(u16 const* a0, nsimd_cpu_vi16 a1);
nsimd_cpu_vu32 NSIMD_VECTORCALL nsimd_gather_cpu_u32(u32 const* a0, nsimd_cpu_vi32 a1);
nsimd_cpu_vi32 NSIMD_VECTORCALL nsimd_gather_cpu_i32(i32 const* a0, nsimd_cpu_vi32 a1);
nsimd_cpu_vi64 NSIMD_VECTORCALL nsimd_gather_cpu_i64(i64 const* a0, nsimd_cpu_vi64 a1);
nsimd_cpu_vu64 NSIMD_VECTORCALL nsimd_gather_cpu_u64(u64 const* a0, nsimd_cpu_vi64 a1);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>nsimd_sve_vf64 NSIMD_VECTORCALL nsimd_gather_sve_f64(f64 const* a0, nsimd_sve_vi64 a1);
nsimd_sve_vf32 NSIMD_VECTORCALL nsimd_gather_sve_f32(f32 const* a0, nsimd_sve_vi32 a1);
nsimd_sve_vf16 NSIMD_VECTORCALL nsimd_gather_sve_f16(f16 const* a0, nsimd_sve_vi16 a1);
nsimd_sve_vi16 NSIMD_VECTORCALL nsimd_gather_sve_i16(i16 const* a0, nsimd_sve_vi16 a1);
nsimd_sve_vu16 NSIMD_VECTORCALL nsimd_gather_sve_u16(u16 const* a0, nsimd_sve_vi16 a1);
nsimd_sve_vu32 NSIMD_VECTORCALL nsimd_gather_sve_u32(u32 const* a0, nsimd_sve_vi32 a1);
nsimd_sve_vi32 NSIMD_VECTORCALL nsimd_gather_sve_i32(i32 const* a0, nsimd_sve_vi32 a1);
nsimd_sve_vi64 NSIMD_VECTORCALL nsimd_gather_sve_i64(i64 const* a0, nsimd_sve_vi64 a1);
nsimd_sve_vu64 NSIMD_VECTORCALL nsimd_gather_sve_u64(u64 const* a0, nsimd_sve_vi64 a1);</code></pre>
<h3>AARCH64</h3>
<pre class="c"><code>nsimd_aarch64_vf64 NSIMD_VECTORCALL nsimd_gather_aarch64_f64(f64 const* a0, nsimd_aarch64_vi64 a1);
nsimd_aarch64_vf32 NSIMD_VECTORCALL nsimd_gather_aarch64_f32(f32 const* a0, nsimd_aarch64_vi32 a1);
nsimd_aarch64_vf16 NSIMD_VECTORCALL nsimd_gather_aarch64_f16(f16 const* a0, nsimd_aarch64_vi16 a1);
nsimd_aarch64_vi16 NSIMD_VECTORCALL nsimd_gather_aarch64_i16(i16 const* a0, nsimd_aarch64_vi16 a1);
nsimd_aarch64_vu16 NSIMD_VECTORCALL nsimd_gather_aarch64_u16(u16 const* a0, nsimd_aarch64_vi16 a1);
nsimd_aarch64_vu32 NSIMD_VECTORCALL nsimd_gather_aarch64_u32(u32 const* a0, nsimd_aarch64_vi32 a1);
nsimd_aarch64_vi32 NSIMD_VECTORCALL nsimd_gather_aarch64_i32(i32 const* a0, nsimd_aarch64_vi32 a1);
nsimd_aarch64_vi64 NSIMD_VECTORCALL nsimd_gather_aarch64_i64(i64 const* a0, nsimd_aarch64_vi64 a1);
nsimd_aarch64_vu64 NSIMD_VECTORCALL nsimd_gather_aarch64_u64(u64 const* a0, nsimd_aarch64_vi64 a1);</code></pre>
<h3>SSE2</h3>
<pre class="c"><code>nsimd_sse2_vf64 NSIMD_VECTORCALL nsimd_gather_sse2_f64(f64 const* a0, nsimd_sse2_vi64 a1);
nsimd_sse2_vf32 NSIMD_VECTORCALL nsimd_gather_sse2_f32(f32 const* a0, nsimd_sse2_vi32 a1);
nsimd_sse2_vf16 NSIMD_VECTORCALL nsimd_gather_sse2_f16(f16 const* a0, nsimd_sse2_vi16 a1);
nsimd_sse2_vi16 NSIMD_VECTORCALL nsimd_gather_sse2_i16(i16 const* a0, nsimd_sse2_vi16 a1);
nsimd_sse2_vu16 NSIMD_VECTORCALL nsimd_gather_sse2_u16(u16 const* a0, nsimd_sse2_vi16 a1);
nsimd_sse2_vu32 NSIMD_VECTORCALL nsimd_gather_sse2_u32(u32 const* a0, nsimd_sse2_vi32 a1);
nsimd_sse2_vi32 NSIMD_VECTORCALL nsimd_gather_sse2_i32(i32 const* a0, nsimd_sse2_vi32 a1);
nsimd_sse2_vi64 NSIMD_VECTORCALL nsimd_gather_sse2_i64(i64 const* a0, nsimd_sse2_vi64 a1);
nsimd_sse2_vu64 NSIMD_VECTORCALL nsimd_gather_sse2_u64(u64 const* a0, nsimd_sse2_vi64 a1);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>nsimd_avx2_vf64 NSIMD_VECTORCALL nsimd_gather_avx2_f64(f64 const* a0, nsimd_avx2_vi64 a1);
nsimd_avx2_vf32 NSIMD_VECTORCALL nsimd_gather_avx2_f32(f32 const* a0, nsimd_avx2_vi32 a1);
nsimd_avx2_vf16 NSIMD_VECTORCALL nsimd_gather_avx2_f16(f16 const* a0, nsimd_avx2_vi16 a1);
nsimd_avx2_vi16 NSIMD_VECTORCALL nsimd_gather_avx2_i16(i16 const* a0, nsimd_avx2_vi16 a1);
nsimd_avx2_vu16 NSIMD_VECTORCALL nsimd_gather_avx2_u16(u16 const* a0, nsimd_avx2_vi16 a1);
nsimd_avx2_vu32 NSIMD_VECTORCALL nsimd_gather_avx2_u32(u32 const* a0, nsimd_avx2_vi32 a1);
nsimd_avx2_vi32 NSIMD_VECTORCALL nsimd_gather_avx2_i32(i32 const* a0, nsimd_avx2_vi32 a1);
nsimd_avx2_vi64 NSIMD_VECTORCALL nsimd_gather_avx2_i64(i64 const* a0, nsimd_avx2_vi64 a1);
nsimd_avx2_vu64 NSIMD_VECTORCALL nsimd_gather_avx2_u64(u64 const* a0, nsimd_avx2_vi64 a1);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>nsimd_avx512_skylake_vf64 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_f64(f64 const* a0, nsimd_avx512_skylake_vi64 a1);
nsimd_avx512_skylake_vf32 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_f32(f32 const* a0, nsimd_avx512_skylake_vi32 a1);
nsimd_avx512_skylake_vf16 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_f16(f16 const* a0, nsimd_avx512_skylake_vi16 a1);
nsimd_avx512_skylake_vi16 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_i16(i16 const* a0, nsimd_avx512_skylake_vi16 a1);
nsimd_avx512_skylake_vu16 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_u16(u16 const* a0, nsimd_avx512_skylake_vi16 a1);
nsimd_avx512_skylake_vu32 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_u32(u32 const* a0, nsimd_avx512_skylake_vi32 a1);
nsimd_avx512_skylake_vi32 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_i32(i32 const* a0, nsimd_avx512_skylake_vi32 a1);
nsimd_avx512_skylake_vi64 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_i64(i64 const* a0, nsimd_avx512_skylake_vi64 a1);
nsimd_avx512_skylake_vu64 NSIMD_VECTORCALL nsimd_gather_avx512_skylake_u64(u64 const* a0, nsimd_avx512_skylake_vi64 a1);</code></pre>
<h3>SVE256</h3>
<pre class="c"><code>nsimd_sve256_vf64 NSIMD_VECTORCALL nsimd_gather_sve256_f64(f64 const* a0, nsimd_sve256_vi64 a1);
nsimd_sve256_vf32 NSIMD_VECTORCALL nsimd_gather_sve256_f32(f32 const* a0, nsimd_sve256_vi32 a1);
nsimd_sve256_vf16 NSIMD_VECTORCALL nsimd_gather_sve256_f16(f16 const* a0, nsimd_sve256_vi16 a1);
nsimd_sve256_vi16 NSIMD_VECTORCALL nsimd_gather_sve256_i16(i16 const* a0, nsimd_sve256_vi16 a1);
nsimd_sve256_vu16 NSIMD_VECTORCALL nsimd_gather_sve256_u16(u16 const* a0, nsimd_sve256_vi16 a1);
nsimd_sve256_vu32 NSIMD_VECTORCALL nsimd_gather_sve256_u32(u32 const* a0, nsimd_sve256_vi32 a1);
nsimd_sve256_vi32 NSIMD_VECTORCALL nsimd_gather_sve256_i32(i32 const* a0, nsimd_sve256_vi32 a1);
nsimd_sve256_vi64 NSIMD_VECTORCALL nsimd_gather_sve256_i64(i64 const* a0, nsimd_sve256_vi64 a1);
nsimd_sve256_vu64 NSIMD_VECTORCALL nsimd_gather_sve256_u64(u64 const* a0, nsimd_sve256_vi64 a1);</code></pre>
<h3>SVE2048</h3>
<pre class="c"><code>nsimd_sve2048_vf64 NSIMD_VECTORCALL nsimd_gather_sve2048_f64(f64 const* a0, nsimd_sve2048_vi64 a1);
nsimd_sve2048_vf32 NSIMD_VECTORCALL nsimd_gather_sve2048_f32(f32 const* a0, nsimd_sve2048_vi32 a1);
nsimd_sve2048_vf16 NSIMD_VECTORCALL nsimd_gather_sve2048_f16(f16 const* a0, nsimd_sve2048_vi16 a1);
nsimd_sve2048_vi16 NSIMD_VECTORCALL nsimd_gather_sve2048_i16(i16 const* a0, nsimd_sve2048_vi16 a1);
nsimd_sve2048_vu16 NSIMD_VECTORCALL nsimd_gather_sve2048_u16(u16 const* a0, nsimd_sve2048_vi16 a1);
nsimd_sve2048_vu32 NSIMD_VECTORCALL nsimd_gather_sve2048_u32(u32 const* a0, nsimd_sve2048_vi32 a1);
nsimd_sve2048_vi32 NSIMD_VECTORCALL nsimd_gather_sve2048_i32(i32 const* a0, nsimd_sve2048_vi32 a1);
nsimd_sve2048_vi64 NSIMD_VECTORCALL nsimd_gather_sve2048_i64(i64 const* a0, nsimd_sve2048_vi64 a1);
nsimd_sve2048_vu64 NSIMD_VECTORCALL nsimd_gather_sve2048_u64(u64 const* a0, nsimd_sve2048_vi64 a1);</code></pre>
<h3>SVE512</h3>
<pre class="c"><code>nsimd_sve512_vf64 NSIMD_VECTORCALL nsimd_gather_sve512_f64(f64 const* a0, nsimd_sve512_vi64 a1);
nsimd_sve512_vf32 NSIMD_VECTORCALL nsimd_gather_sve512_f32(f32 const* a0, nsimd_sve512_vi32 a1);
nsimd_sve512_vf16 NSIMD_VECTORCALL nsimd_gather_sve512_f16(f16 const* a0, nsimd_sve512_vi16 a1);
nsimd_sve512_vi16 NSIMD_VECTORCALL nsimd_gather_sve512_i16(i16 const* a0, nsimd_sve512_vi16 a1);
nsimd_sve512_vu16 NSIMD_VECTORCALL nsimd_gather_sve512_u16(u16 const* a0, nsimd_sve512_vi16 a1);
nsimd_sve512_vu32 NSIMD_VECTORCALL nsimd_gather_sve512_u32(u32 const* a0, nsimd_sve512_vi32 a1);
nsimd_sve512_vi32 NSIMD_VECTORCALL nsimd_gather_sve512_i32(i32 const* a0, nsimd_sve512_vi32 a1);
nsimd_sve512_vi64 NSIMD_VECTORCALL nsimd_gather_sve512_i64(i64 const* a0, nsimd_sve512_vi64 a1);
nsimd_sve512_vu64 NSIMD_VECTORCALL nsimd_gather_sve512_u64(u64 const* a0, nsimd_sve512_vi64 a1);</code></pre>
<h3>SVE1024</h3>
<pre class="c"><code>nsimd_sve1024_vf64 NSIMD_VECTORCALL nsimd_gather_sve1024_f64(f64 const* a0, nsimd_sve1024_vi64 a1);
nsimd_sve1024_vf32 NSIMD_VECTORCALL nsimd_gather_sve1024_f32(f32 const* a0, nsimd_sve1024_vi32 a1);
nsimd_sve1024_vf16 NSIMD_VECTORCALL nsimd_gather_sve1024_f16(f16 const* a0, nsimd_sve1024_vi16 a1);
nsimd_sve1024_vi16 NSIMD_VECTORCALL nsimd_gather_sve1024_i16(i16 const* a0, nsimd_sve1024_vi16 a1);
nsimd_sve1024_vu16 NSIMD_VECTORCALL nsimd_gather_sve1024_u16(u16 const* a0, nsimd_sve1024_vi16 a1);
nsimd_sve1024_vu32 NSIMD_VECTORCALL nsimd_gather_sve1024_u32(u32 const* a0, nsimd_sve1024_vi32 a1);
nsimd_sve1024_vi32 NSIMD_VECTORCALL nsimd_gather_sve1024_i32(i32 const* a0, nsimd_sve1024_vi32 a1);
nsimd_sve1024_vi64 NSIMD_VECTORCALL nsimd_gather_sve1024_i64(i64 const* a0, nsimd_sve1024_vi64 a1);
nsimd_sve1024_vu64 NSIMD_VECTORCALL nsimd_gather_sve1024_u64(u64 const* a0, nsimd_sve1024_vi64 a1);</code></pre>
<h2>C++ base API (architecture specifics)</h2>
<h3>SVE128</h3>
<pre class="c"><code>nsimd_sve128_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve128_vi64 a1, f64, sve128);
nsimd_sve128_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve128_vi32 a1, f32, sve128);
nsimd_sve128_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve128_vi16 a1, f16, sve128);
nsimd_sve128_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve128_vi16 a1, i16, sve128);
nsimd_sve128_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve128_vi16 a1, u16, sve128);
nsimd_sve128_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve128_vi32 a1, u32, sve128);
nsimd_sve128_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve128_vi32 a1, i32, sve128);
nsimd_sve128_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve128_vi64 a1, i64, sve128);
nsimd_sve128_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve128_vi64 a1, u64, sve128);</code></pre>
<h3>POWER7</h3>
<pre class="c"><code>nsimd_power7_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_power7_vi64 a1, f64, power7);
nsimd_power7_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_power7_vi32 a1, f32, power7);
nsimd_power7_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_power7_vi16 a1, f16, power7);
nsimd_power7_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_power7_vi16 a1, i16, power7);
nsimd_power7_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_power7_vi16 a1, u16, power7);
nsimd_power7_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_power7_vi32 a1, u32, power7);
nsimd_power7_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_power7_vi32 a1, i32, power7);
nsimd_power7_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_power7_vi64 a1, i64, power7);
nsimd_power7_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_power7_vi64 a1, u64, power7);</code></pre>
<h3>AVX</h3>
<pre class="c"><code>nsimd_avx_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_avx_vi64 a1, f64, avx);
nsimd_avx_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_avx_vi32 a1, f32, avx);
nsimd_avx_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_avx_vi16 a1, f16, avx);
nsimd_avx_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_avx_vi16 a1, i16, avx);
nsimd_avx_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_avx_vi16 a1, u16, avx);
nsimd_avx_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_avx_vi32 a1, u32, avx);
nsimd_avx_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_avx_vi32 a1, i32, avx);
nsimd_avx_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_avx_vi64 a1, i64, avx);
nsimd_avx_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_avx_vi64 a1, u64, avx);</code></pre>
<h3>SSE42</h3>
<pre class="c"><code>nsimd_sse42_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sse42_vi64 a1, f64, sse42);
nsimd_sse42_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sse42_vi32 a1, f32, sse42);
nsimd_sse42_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sse42_vi16 a1, f16, sse42);
nsimd_sse42_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sse42_vi16 a1, i16, sse42);
nsimd_sse42_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sse42_vi16 a1, u16, sse42);
nsimd_sse42_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sse42_vi32 a1, u32, sse42);
nsimd_sse42_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sse42_vi32 a1, i32, sse42);
nsimd_sse42_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sse42_vi64 a1, i64, sse42);
nsimd_sse42_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sse42_vi64 a1, u64, sse42);</code></pre>
<h3>NEON128</h3>
<pre class="c"><code>nsimd_neon128_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_neon128_vi64 a1, f64, neon128);
nsimd_neon128_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_neon128_vi32 a1, f32, neon128);
nsimd_neon128_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_neon128_vi16 a1, f16, neon128);
nsimd_neon128_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_neon128_vi16 a1, i16, neon128);
nsimd_neon128_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_neon128_vi16 a1, u16, neon128);
nsimd_neon128_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_neon128_vi32 a1, u32, neon128);
nsimd_neon128_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_neon128_vi32 a1, i32, neon128);
nsimd_neon128_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_neon128_vi64 a1, i64, neon128);
nsimd_neon128_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_neon128_vi64 a1, u64, neon128);</code></pre>
<h3>POWER8</h3>
<pre class="c"><code>nsimd_power8_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_power8_vi64 a1, f64, power8);
nsimd_power8_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_power8_vi32 a1, f32, power8);
nsimd_power8_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_power8_vi16 a1, f16, power8);
nsimd_power8_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_power8_vi16 a1, i16, power8);
nsimd_power8_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_power8_vi16 a1, u16, power8);
nsimd_power8_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_power8_vi32 a1, u32, power8);
nsimd_power8_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_power8_vi32 a1, i32, power8);
nsimd_power8_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_power8_vi64 a1, i64, power8);
nsimd_power8_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_power8_vi64 a1, u64, power8);</code></pre>
<h3>AVX512_KNL</h3>
<pre class="c"><code>nsimd_avx512_knl_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_avx512_knl_vi64 a1, f64, avx512_knl);
nsimd_avx512_knl_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_avx512_knl_vi32 a1, f32, avx512_knl);
nsimd_avx512_knl_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_avx512_knl_vi16 a1, f16, avx512_knl);
nsimd_avx512_knl_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_avx512_knl_vi16 a1, i16, avx512_knl);
nsimd_avx512_knl_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_avx512_knl_vi16 a1, u16, avx512_knl);
nsimd_avx512_knl_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_avx512_knl_vi32 a1, u32, avx512_knl);
nsimd_avx512_knl_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_avx512_knl_vi32 a1, i32, avx512_knl);
nsimd_avx512_knl_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_avx512_knl_vi64 a1, i64, avx512_knl);
nsimd_avx512_knl_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_avx512_knl_vi64 a1, u64, avx512_knl);</code></pre>
<h3>CPU</h3>
<pre class="c"><code>nsimd_cpu_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_cpu_vi64 a1, f64, cpu);
nsimd_cpu_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_cpu_vi32 a1, f32, cpu);
nsimd_cpu_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_cpu_vi16 a1, f16, cpu);
nsimd_cpu_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_cpu_vi16 a1, i16, cpu);
nsimd_cpu_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_cpu_vi16 a1, u16, cpu);
nsimd_cpu_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_cpu_vi32 a1, u32, cpu);
nsimd_cpu_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_cpu_vi32 a1, i32, cpu);
nsimd_cpu_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_cpu_vi64 a1, i64, cpu);
nsimd_cpu_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_cpu_vi64 a1, u64, cpu);</code></pre>
<h3>SVE</h3>
<pre class="c"><code>nsimd_sve_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve_vi64 a1, f64, sve);
nsimd_sve_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve_vi32 a1, f32, sve);
nsimd_sve_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve_vi16 a1, f16, sve);
nsimd_sve_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve_vi16 a1, i16, sve);
nsimd_sve_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve_vi16 a1, u16, sve);
nsimd_sve_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve_vi32 a1, u32, sve);
nsimd_sve_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve_vi32 a1, i32, sve);
nsimd_sve_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve_vi64 a1, i64, sve);
nsimd_sve_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve_vi64 a1, u64, sve);</code></pre>
<h3>AARCH64</h3>
<pre class="c"><code>nsimd_aarch64_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_aarch64_vi64 a1, f64, aarch64);
nsimd_aarch64_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_aarch64_vi32 a1, f32, aarch64);
nsimd_aarch64_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_aarch64_vi16 a1, f16, aarch64);
nsimd_aarch64_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_aarch64_vi16 a1, i16, aarch64);
nsimd_aarch64_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_aarch64_vi16 a1, u16, aarch64);
nsimd_aarch64_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_aarch64_vi32 a1, u32, aarch64);
nsimd_aarch64_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_aarch64_vi32 a1, i32, aarch64);
nsimd_aarch64_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_aarch64_vi64 a1, i64, aarch64);
nsimd_aarch64_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_aarch64_vi64 a1, u64, aarch64);</code></pre>
<h3>SSE2</h3>
<pre class="c"><code>nsimd_sse2_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sse2_vi64 a1, f64, sse2);
nsimd_sse2_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sse2_vi32 a1, f32, sse2);
nsimd_sse2_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sse2_vi16 a1, f16, sse2);
nsimd_sse2_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sse2_vi16 a1, i16, sse2);
nsimd_sse2_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sse2_vi16 a1, u16, sse2);
nsimd_sse2_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sse2_vi32 a1, u32, sse2);
nsimd_sse2_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sse2_vi32 a1, i32, sse2);
nsimd_sse2_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sse2_vi64 a1, i64, sse2);
nsimd_sse2_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sse2_vi64 a1, u64, sse2);</code></pre>
<h3>AVX2</h3>
<pre class="c"><code>nsimd_avx2_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_avx2_vi64 a1, f64, avx2);
nsimd_avx2_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_avx2_vi32 a1, f32, avx2);
nsimd_avx2_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_avx2_vi16 a1, f16, avx2);
nsimd_avx2_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_avx2_vi16 a1, i16, avx2);
nsimd_avx2_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_avx2_vi16 a1, u16, avx2);
nsimd_avx2_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_avx2_vi32 a1, u32, avx2);
nsimd_avx2_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_avx2_vi32 a1, i32, avx2);
nsimd_avx2_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_avx2_vi64 a1, i64, avx2);
nsimd_avx2_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_avx2_vi64 a1, u64, avx2);</code></pre>
<h3>AVX512_SKYLAKE</h3>
<pre class="c"><code>nsimd_avx512_skylake_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_avx512_skylake_vi64 a1, f64, avx512_skylake);
nsimd_avx512_skylake_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_avx512_skylake_vi32 a1, f32, avx512_skylake);
nsimd_avx512_skylake_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_avx512_skylake_vi16 a1, f16, avx512_skylake);
nsimd_avx512_skylake_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_avx512_skylake_vi16 a1, i16, avx512_skylake);
nsimd_avx512_skylake_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_avx512_skylake_vi16 a1, u16, avx512_skylake);
nsimd_avx512_skylake_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_avx512_skylake_vi32 a1, u32, avx512_skylake);
nsimd_avx512_skylake_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_avx512_skylake_vi32 a1, i32, avx512_skylake);
nsimd_avx512_skylake_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_avx512_skylake_vi64 a1, i64, avx512_skylake);
nsimd_avx512_skylake_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_avx512_skylake_vi64 a1, u64, avx512_skylake);</code></pre>
<h3>SVE256</h3>
<pre class="c"><code>nsimd_sve256_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve256_vi64 a1, f64, sve256);
nsimd_sve256_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve256_vi32 a1, f32, sve256);
nsimd_sve256_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve256_vi16 a1, f16, sve256);
nsimd_sve256_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve256_vi16 a1, i16, sve256);
nsimd_sve256_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve256_vi16 a1, u16, sve256);
nsimd_sve256_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve256_vi32 a1, u32, sve256);
nsimd_sve256_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve256_vi32 a1, i32, sve256);
nsimd_sve256_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve256_vi64 a1, i64, sve256);
nsimd_sve256_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve256_vi64 a1, u64, sve256);</code></pre>
<h3>SVE2048</h3>
<pre class="c"><code>nsimd_sve2048_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve2048_vi64 a1, f64, sve2048);
nsimd_sve2048_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve2048_vi32 a1, f32, sve2048);
nsimd_sve2048_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve2048_vi16 a1, f16, sve2048);
nsimd_sve2048_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve2048_vi16 a1, i16, sve2048);
nsimd_sve2048_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve2048_vi16 a1, u16, sve2048);
nsimd_sve2048_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve2048_vi32 a1, u32, sve2048);
nsimd_sve2048_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve2048_vi32 a1, i32, sve2048);
nsimd_sve2048_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve2048_vi64 a1, i64, sve2048);
nsimd_sve2048_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve2048_vi64 a1, u64, sve2048);</code></pre>
<h3>SVE512</h3>
<pre class="c"><code>nsimd_sve512_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve512_vi64 a1, f64, sve512);
nsimd_sve512_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve512_vi32 a1, f32, sve512);
nsimd_sve512_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve512_vi16 a1, f16, sve512);
nsimd_sve512_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve512_vi16 a1, i16, sve512);
nsimd_sve512_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve512_vi16 a1, u16, sve512);
nsimd_sve512_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve512_vi32 a1, u32, sve512);
nsimd_sve512_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve512_vi32 a1, i32, sve512);
nsimd_sve512_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve512_vi64 a1, i64, sve512);
nsimd_sve512_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve512_vi64 a1, u64, sve512);</code></pre>
<h3>SVE1024</h3>
<pre class="c"><code>nsimd_sve1024_vf64 NSIMD_VECTORCALL gather(f64 const* a0, nsimd_sve1024_vi64 a1, f64, sve1024);
nsimd_sve1024_vf32 NSIMD_VECTORCALL gather(f32 const* a0, nsimd_sve1024_vi32 a1, f32, sve1024);
nsimd_sve1024_vf16 NSIMD_VECTORCALL gather(f16 const* a0, nsimd_sve1024_vi16 a1, f16, sve1024);
nsimd_sve1024_vi16 NSIMD_VECTORCALL gather(i16 const* a0, nsimd_sve1024_vi16 a1, i16, sve1024);
nsimd_sve1024_vu16 NSIMD_VECTORCALL gather(u16 const* a0, nsimd_sve1024_vi16 a1, u16, sve1024);
nsimd_sve1024_vu32 NSIMD_VECTORCALL gather(u32 const* a0, nsimd_sve1024_vi32 a1, u32, sve1024);
nsimd_sve1024_vi32 NSIMD_VECTORCALL gather(i32 const* a0, nsimd_sve1024_vi32 a1, i32, sve1024);
nsimd_sve1024_vi64 NSIMD_VECTORCALL gather(i64 const* a0, nsimd_sve1024_vi64 a1, i64, sve1024);
nsimd_sve1024_vu64 NSIMD_VECTORCALL gather(u64 const* a0, nsimd_sve1024_vi64 a1, u64, sve1024);</code></pre>
  </body>
</html>
